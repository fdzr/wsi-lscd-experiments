{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9284b30-03fb-455f-80fe-53d2811d3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df1db3bf-ee47-4305-b7db-8df38c33fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_annotations(l, min_annots=3, min_agreed=3):\n",
    "    '''\n",
    "    Leaves only uses annotated at least by min_annots annotators, among which \n",
    "    at least min_agreed returned the same sense.\n",
    "    '''\n",
    "    if len(l) < min_annots:\n",
    "        return None\n",
    "    # take the most frequent answer, or None if less than min_agreed annotators agreed\n",
    "    # in case of tie, among the most frequent answers most_common() returns the first appeared one \n",
    "    label, cnt = Counter(l).most_common(1)[0]\n",
    "    return None if cnt < min_agreed else label\n",
    "\n",
    "def load_sense_labels(judgments_senses_path):\n",
    "    '''\n",
    "    Loads sense annotations, aggregates annotations by multiple annotators for a singe use.\n",
    "    '''\n",
    "    df = pd.read_csv(judgments_senses_path, delimiter=\"\\t\")\n",
    "    senses = pd.read_csv(judgments_senses_path.parent / 'senses.csv', delimiter=\"\\t\")\n",
    "    df = pd.merge(df, senses, on='identifier_sense')\n",
    "    df = df[~(df['description_sense'] == 'andere')] # 'andere' stands for 'all other senses'\n",
    "    clusters = df.groupby('identifier')['identifier_sense'].apply(agg_annotations).dropna().reset_index()\n",
    "    clusters = clusters.rename(columns={'identifier_sense': 'cluster'})\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b43674c-04cd-40bd-a286-542a540865c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = 'de'\n",
    "DIR_NAME = f'dwug_{LANG}'\n",
    "CLUSTERS_NAME = 'sense'\n",
    "p = Path(DIR_NAME)\n",
    "\n",
    "if CLUSTERS_NAME == 'sense':\n",
    "    paths = p.glob(f'data/*/judgments_senses.csv')\n",
    "    clusters = pd.concat([load_sense_labels(path) for path in paths], ignore_index=True)\n",
    "else:\n",
    "    paths = p.glob(f'clusters/{CLUSTERS_NAME}/*.csv')\n",
    "    clusters = pd.concat([pd.read_csv(path,delimiter=\"\\t\", quoting=csv.QUOTE_NONE) for path in paths], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55644d04-3612-4445-8fb3-8e383724f2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9125 uses loaded, 826 have gold labels\n",
      "Uses loaded: {'new': 5000, 'old': 4125}\n",
      "Uses with gold labels: {'new': 437, 'old': 389}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>cluster</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>date</th>\n",
       "      <th>grouping</th>\n",
       "      <th>description</th>\n",
       "      <th>context</th>\n",
       "      <th>indexes_target_token</th>\n",
       "      <th>indexes_target_sentence</th>\n",
       "      <th>context_tokenized</th>\n",
       "      <th>indexes_target_token_tokenized</th>\n",
       "      <th>indexes_target_sentence_tokenized</th>\n",
       "      <th>context_lemmatized</th>\n",
       "      <th>context_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2532889X_1961-04-10_01_051.tcf.xml-4-2</td>\n",
       "      <td>sense3</td>\n",
       "      <td>überspannen</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>1961</td>\n",
       "      <td>new</td>\n",
       "      <td></td>\n",
       "      <td>Der Handlungsbogen überspannt vier Jahrzehnte,...</td>\n",
       "      <td>19:29</td>\n",
       "      <td>0:304</td>\n",
       "      <td>Der Handlungsbogen überspannt vier Jahrzehnte ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0:53</td>\n",
       "      <td>d Handlungsbogen überspannen vier Jahrzehnt , ...</td>\n",
       "      <td>ART NN VVFIN CARD NN $, VVFIN APPRART NN ART A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2532889X_1964-11-23_01_007.tcf.xml-3-10</td>\n",
       "      <td>sense3</td>\n",
       "      <td>überspannen</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>1964</td>\n",
       "      <td>new</td>\n",
       "      <td></td>\n",
       "      <td>Die Brücke, die Insgesamt 4,8 Kilometer lang i...</td>\n",
       "      <td>50:60</td>\n",
       "      <td>0:243</td>\n",
       "      <td>Die Brücke , die Insgesamt 4,8 Kilometer lang ...</td>\n",
       "      <td>10</td>\n",
       "      <td>0:41</td>\n",
       "      <td>d Brücke , die insgesamt 4,8 Kilometer lang se...</td>\n",
       "      <td>ART NN $, PRELS ADV CARD NN ADJD VAFIN $, VVFI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2532889X_1975-10-22_01_155.tcf.xml-2-23</td>\n",
       "      <td>sense3</td>\n",
       "      <td>überspannen</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>1975</td>\n",
       "      <td>new</td>\n",
       "      <td></td>\n",
       "      <td>• Mit der Fertigstellung der ersten 143 Meter,...</td>\n",
       "      <td>140:150</td>\n",
       "      <td>0:248</td>\n",
       "      <td>• Mit der Fertigstellung der ersten 143 Meter ...</td>\n",
       "      <td>23</td>\n",
       "      <td>0:38</td>\n",
       "      <td>• mit d Fertigstellung d erst 143 Meter , lang...</td>\n",
       "      <td>$( APPR ART NN ART ADJA CARD NN $, ADJA NN $, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2532889X_1975-11-24_01_038.tcf.xml-2-22</td>\n",
       "      <td>sense3</td>\n",
       "      <td>überspannen</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>1975</td>\n",
       "      <td>new</td>\n",
       "      <td></td>\n",
       "      <td>Eine 460 Meter lange Brücke, die das künftige ...</td>\n",
       "      <td>117:127</td>\n",
       "      <td>0:209</td>\n",
       "      <td>Eine 460 Meter lange Brücke , die das künftige...</td>\n",
       "      <td>22</td>\n",
       "      <td>0:36</td>\n",
       "      <td>eine 460 Meter lang Brücke , die d künftig neu...</td>\n",
       "      <td>ART CARD NN ADJA NN $, PRELS ART ADJA ADJA NN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2532889X_1980-05-10_01_109.tcf.xml-5-7</td>\n",
       "      <td>sense3</td>\n",
       "      <td>überspannen</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>1980</td>\n",
       "      <td>new</td>\n",
       "      <td></td>\n",
       "      <td>Die Kadin-Brücke wurde 1470 fertiggestellt, si...</td>\n",
       "      <td>48:58</td>\n",
       "      <td>0:149</td>\n",
       "      <td>Die Kadin-Brücke wurde 1470 fertiggestellt , s...</td>\n",
       "      <td>7</td>\n",
       "      <td>0:26</td>\n",
       "      <td>d Kadin-Brücke werden 1470 fertigstellen , sie...</td>\n",
       "      <td>ART NN VAFIN CARD VVPP $, PPER VVFIN APPR ART ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                identifier cluster        lemma    pos  date  \\\n",
       "0   2532889X_1961-04-10_01_051.tcf.xml-4-2  sense3  überspannen  VVFIN  1961   \n",
       "1  2532889X_1964-11-23_01_007.tcf.xml-3-10  sense3  überspannen  VVFIN  1964   \n",
       "2  2532889X_1975-10-22_01_155.tcf.xml-2-23  sense3  überspannen  VVFIN  1975   \n",
       "3  2532889X_1975-11-24_01_038.tcf.xml-2-22  sense3  überspannen  VVFIN  1975   \n",
       "4   2532889X_1980-05-10_01_109.tcf.xml-5-7  sense3  überspannen  VVFIN  1980   \n",
       "\n",
       "  grouping description                                            context  \\\n",
       "0      new              Der Handlungsbogen überspannt vier Jahrzehnte,...   \n",
       "1      new              Die Brücke, die Insgesamt 4,8 Kilometer lang i...   \n",
       "2      new              • Mit der Fertigstellung der ersten 143 Meter,...   \n",
       "3      new              Eine 460 Meter lange Brücke, die das künftige ...   \n",
       "4      new              Die Kadin-Brücke wurde 1470 fertiggestellt, si...   \n",
       "\n",
       "  indexes_target_token indexes_target_sentence  \\\n",
       "0                19:29                   0:304   \n",
       "1                50:60                   0:243   \n",
       "2              140:150                   0:248   \n",
       "3              117:127                   0:209   \n",
       "4                48:58                   0:149   \n",
       "\n",
       "                                   context_tokenized  \\\n",
       "0  Der Handlungsbogen überspannt vier Jahrzehnte ...   \n",
       "1  Die Brücke , die Insgesamt 4,8 Kilometer lang ...   \n",
       "2  • Mit der Fertigstellung der ersten 143 Meter ...   \n",
       "3  Eine 460 Meter lange Brücke , die das künftige...   \n",
       "4  Die Kadin-Brücke wurde 1470 fertiggestellt , s...   \n",
       "\n",
       "   indexes_target_token_tokenized indexes_target_sentence_tokenized  \\\n",
       "0                               2                              0:53   \n",
       "1                              10                              0:41   \n",
       "2                              23                              0:38   \n",
       "3                              22                              0:36   \n",
       "4                               7                              0:26   \n",
       "\n",
       "                                  context_lemmatized  \\\n",
       "0  d Handlungsbogen überspannen vier Jahrzehnt , ...   \n",
       "1  d Brücke , die insgesamt 4,8 Kilometer lang se...   \n",
       "2  • mit d Fertigstellung d erst 143 Meter , lang...   \n",
       "3  eine 460 Meter lang Brücke , die d künftig neu...   \n",
       "4  d Kadin-Brücke werden 1470 fertigstellen , sie...   \n",
       "\n",
       "                                         context_pos  \n",
       "0  ART NN VVFIN CARD NN $, VVFIN APPRART NN ART A...  \n",
       "1  ART NN $, PRELS ADV CARD NN ADJD VAFIN $, VVFI...  \n",
       "2  $( APPR ART NN ART ADJA CARD NN $, ADJA NN $, ...  \n",
       "3  ART CARD NN ADJA NN $, PRELS ART ADJA ADJA NN ...  \n",
       "4  ART NN VAFIN CARD VVPP $, PPER VVFIN APPR ART ...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = p.glob('data/*/uses.csv')\n",
    "uses = pd.concat([pd.read_csv(path,delimiter=\"\\t\", quoting=csv.QUOTE_NONE) for path in paths], ignore_index=True)\n",
    "uses['grouping'] = uses.grouping.replace(1,'old').replace(2,'new')\n",
    "rdf = clusters.merge(uses, on='identifier', how='inner', validate='1:1')\n",
    "assert len(clusters)==len(rdf)\n",
    "print(f'{len(uses)} uses loaded, {len(rdf)} have gold labels')\n",
    "print('Uses loaded:',uses.grouping.value_counts().to_dict())\n",
    "print('Uses with gold labels:',rdf.grouping.value_counts().to_dict())\n",
    "rdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5180c40-2d44-4be8-af25-b5d5297d23c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_save(df, fpath):\n",
    "    res = pd.DataFrame({\n",
    "        'context_id': df.identifier,\n",
    "        'word': df.lemma,\n",
    "        'gold_sense_id': df.cluster,\n",
    "        'positions': df.indexes_target_token.str.replace(\":\",\"-\"),\n",
    "        'context': df.context\n",
    "    })\n",
    "    print(len(res), fpath)\n",
    "    fpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    res.to_csv(fpath, sep='\\t', index=False, quoting=csv.QUOTE_MINIMAL, quotechar='\"', doublequote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "be172eb3-af2e-4e0d-9a0e-d643d907fd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389 de/sense-old.tsv\n",
      "437 de/sense-new.tsv\n",
      "826 de/sense-old+new.tsv\n",
      "4125 ../datasets_unlabeled/se20lscd/de/sense-old.tsv\n",
      "5000 ../datasets_unlabeled/se20lscd/de/sense-new.tsv\n",
      "9125 ../datasets_unlabeled/se20lscd/de/sense-old+new.tsv\n"
     ]
    }
   ],
   "source": [
    "uses['cluster']=None\n",
    "\n",
    "for df, path in [(rdf, p.parent), (uses, p.parent / '../datasets_unlabeled/se20lscd')]:\n",
    "    for pdf, part in [(df.query('grouping==\"old\"'), 'old'), (df.query('grouping==\"new\"'), 'new'), (df, 'old+new')]:\n",
    "        mask = pdf[\"indexes_target_token\"].str.len() > 2  # old code, not sure if we need it\n",
    "        assert mask.all(), pdf[~mask]\n",
    "        convert_save(pdf, path / LANG / f'{CLUSTERS_NAME}-{part}.tsv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b3f8f-39a3-4e09-a4f7-f412a0b5a7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ba3e5-ee6f-4c43-b046-19c4de3fb6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(dest=\"dir_name\")\n",
    "parser.add_argument(dest=\"clusters\")\n",
    "parser.add_argument(dest=\"output_path\")\n",
    "parser.add_argument('--grouping', action='store_true')\n",
    "parser.add_argument('--split_groupings', action='store_true')\n",
    "args = parser.parse_args()\n",
    "DIR_NAME = args.dir_name\n",
    "CLUSTERS_NAME = args.clusters\n",
    "OUTPUT_PATH = args.output_path\n",
    "GROUPING = args.grouping\n",
    "SPLIT = args.split_groupings\n",
    "\n",
    "if SPLIT and not GROUPING:\n",
    "    print('Could not split into groupings without --grouping flags set!')\n",
    "    exit()\n",
    "\n",
    "\n",
    "data = pd.DataFrame()\n",
    "if CLUSTERS_NAME != \"sense\":\n",
    "    words = [filename[:-4] for filename in os.listdir(\"{}/clusters/{}\".format(DIR_NAME, CLUSTERS_NAME))]\n",
    "else:\n",
    "    words = [filename for filename in os.listdir(\"{}/data\".format(DIR_NAME)) if os.path.exists(\"{}/data/{}/judgments_senses.csv\".format(DIR_NAME, filename))]\n",
    "for word in sorted(words):\n",
    "    try:\n",
    "        sentenses = pd.read_csv(\"{}/data/{}/uses.csv\".format(DIR_NAME, word), delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    except Exception as ex:\n",
    "        print(\"can't download data from {}      {}\".format(\"{}/data/{}/uses.csv\".format(DIR_NAME, word), ex))\n",
    "        continue\n",
    "    if CLUSTERS_NAME != \"sense\":\n",
    "        try:\n",
    "            clusters = pd.read_csv(\"{}/clusters/{}/{}.csv\".format(DIR_NAME, CLUSTERS_NAME, word).format(word), delimiter=\"\\t\")\n",
    "        except Exception as ex:\n",
    "            print(\"can't download data from {}      {}\".format(\"{}/clusters/{}/{}.csv\".format(DIR_NAME, CLUSTERS_NAME, word), ex))\n",
    "            continue\n",
    "        chunk_of_data = pd.merge(sentenses, clusters, left_on='identifier', right_on='identifier')\n",
    "    else:\n",
    "        try:\n",
    "            threshold = 3 # threshold for majority labels\n",
    "            clusters = pd.read_csv(\"{}/data/{}/judgments_senses.csv\".format(DIR_NAME, word), delimiter=\"\\t\")\n",
    "            senses = pd.read_csv(\"{}/data/{}/senses.csv\".format(DIR_NAME, word), delimiter=\"\\t\")\n",
    "            clusters = pd.merge(clusters, senses, left_on='identifier_sense', right_on=\"identifier_sense\")\n",
    "            clusters = clusters[~(clusters['description_sense'] == 'andere')] # remove andereinstances\n",
    "            lemmas = clusters.groupby('identifier').agg({'lemma':lambda x: list(x)[0]})\n",
    "            judgments = clusters.groupby('identifier')['identifier_sense'].apply(list).reset_index(name='judgments')\n",
    "            clusters = pd.merge(lemmas, judgments, left_on='identifier', right_on=\"identifier\")\n",
    "            \n",
    "            # Extract majority labels\n",
    "            def extract_majority_label(judgments, threshold):\n",
    "                judgments = list(judgments)\n",
    "                label2count = Counter(judgments)\n",
    "                majority_labels = [l for l, c in label2count.items() if c >= threshold]\n",
    "                if len(majority_labels) > 0:\n",
    "                    label = np.random.choice(majority_labels)\n",
    "                else:\n",
    "                    label = np.NaN  \n",
    "                return label\n",
    "            \n",
    "            #clusters = clusters[clusters['judgments'].apply(lambda x: len(list(x))>threshold)] # remove instances with less than threshold remaining judgments, not needed for now\n",
    "            #clusters = clusters[~clusters['judgments'].apply(lambda x: extract_majority_label(list(x), threshold)).isnull()] # remove instances which do not reach threshold for majority labeling, not needed for now\n",
    "            clusters['identifier_sense'] = clusters['judgments'].apply(lambda x: extract_majority_label(list(x), threshold)) # add majority label column\n",
    "            clusters = clusters[~clusters['identifier_sense'].isnull()] # remove instances which do not reach threshold for majority labeling\n",
    "            #print(clusters)\n",
    "        except Exception as ex:\n",
    "            print(\"can't download data from {}      {}\".format(\"{}/data/{}/judgments_senses.csv\".format(DIR_NAME, word), ex))\n",
    "            continue\n",
    "        chunk_of_data = pd.merge(sentenses, clusters[[\"identifier\", \"identifier_sense\"]], left_on='identifier', right_on='identifier')\n",
    "        identifier_sense_to_id_mapping = {ident: idx for idx, ident in enumerate(pd.unique(chunk_of_data['identifier_sense']))}\n",
    "        chunk_of_data[\"cluster\"] = chunk_of_data['identifier_sense'].apply(lambda x: identifier_sense_to_id_mapping[x])\n",
    "    data = pd.concat([data, chunk_of_data], ignore_index=True)\n",
    "\n",
    "data[\"indexes_target_token\"] = data[\"indexes_target_token\"].str.replace(\":\", \"-\")\n",
    "data = data[data[\"indexes_target_token\"].str.len() > 2]\n",
    "if GROUPING:\n",
    "    bts_rnc_like_data = pd.DataFrame(\n",
    "        dict(\n",
    "            context_id=range(1, len(data['lemma']) + 1),\n",
    "            word=data['lemma'],\n",
    "            gold_sense_id=data['cluster'],\n",
    "            positions=data[\"indexes_target_token\"],\n",
    "            context=data['context'],\n",
    "            grouping=data['grouping']))\n",
    "    if SPLIT:\n",
    "        grpgs = bts_rnc_like_data['grouping'].unique()\n",
    "        base_path = OUTPUT_PATH[:-4]\n",
    "        extension = OUTPUT_PATH[-4:]\n",
    "        if len(grpgs) > 1:\n",
    "            for grp in grpgs:\n",
    "                to_save = bts_rnc_like_data[bts_rnc_like_data['grouping']==grp]\n",
    "                to_save.to_csv(base_path+'_'+str(grp)+extension, sep='\\t', index=False, quoting=csv.QUOTE_MINIMAL, quotechar='\"', doublequote=True)\n",
    "else:\n",
    "    bts_rnc_like_data = pd.DataFrame(\n",
    "        dict(\n",
    "            context_id=range(1, len(data['lemma']) + 1),\n",
    "            word=data['lemma'],\n",
    "            gold_sense_id=data['cluster'],\n",
    "            positions=data[\"indexes_target_token\"],\n",
    "            context=data['context']))\n",
    "bts_rnc_like_data.to_csv(OUTPUT_PATH, sep='\\t', index=False, quoting=csv.QUOTE_MINIMAL, quotechar='\"', doublequote=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
